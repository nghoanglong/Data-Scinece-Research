{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bytes Pair Encoding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLuD/6KwjY5MLtfPSFBzAJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nghoanglong/Data-Science-Research/blob/master/NLP%20Research/Bytes_Pair_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAexi1Ie-E3p"
      },
      "source": [
        "import collections\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-k6D0RnMd20"
      },
      "source": [
        "!wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49fyEXaXQOQi"
      },
      "source": [
        "def get_corpus(PATH_FILE):\n",
        "    \"\"\"Lấy ra các word và format theo BPE\n",
        "\n",
        "    Mọi word sẽ được format về dạng word('char char char </w>')\n",
        "    \n",
        "    word <- 'char char char </w>'\n",
        "    num <- số lần xuất hiện của word đó trong toàn corpus \n",
        "\n",
        "    return dict([word_1: num, word_2: num,...])\n",
        "    \"\"\"\n",
        "    corpus = collections.defaultdict(int)\n",
        "    with open(PATH_FILE, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            charaters = line.strip().split()\n",
        "            for char in charaters:\n",
        "                corpus[' '.join(char) + '</w>'] += 1\n",
        "    return corpus\n",
        "\n",
        "def get_stats(corpus):\n",
        "    \"\"\"Thống kê tần suất theo cặp của toàn bộ character trong corpus\n",
        "\n",
        "        return defaultdict(int,\n",
        "                           {('char', 'char'): int,\n",
        "                            ('char', 'char'): int,\n",
        "                            (...)})\n",
        "    \"\"\"\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in corpus.items():\n",
        "        char = word.split()\n",
        "        for i in range(len(char) - 1):\n",
        "            pairs[char[i], char[i + 1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def update_corpus(pair, corpus):\n",
        "    \"\"\"Gộp cặp character có tần suất xuất hiện nhiều nhất và update trên toàn corpus\n",
        "\n",
        "       pair: cặp character có tần suất xuất hiện lớn nhất trong corpus\n",
        "       corpus: chứa các word và frequency của word đó\n",
        "\n",
        "       return new_corpus sau khi đã merge cặp character này trên tất cả các word của corpus\n",
        "    \"\"\"\n",
        "    new_corpus = collections.defaultdict(int)\n",
        "    format_char = re.escape(' '.join(pair)) \n",
        "    pattern = re.compile(r\"(?<!\\S)\" + format_char + r\"(?!\\S)\") # 2 kí tự đứng trước và sau format_char phải là khoảng trắng(space) -> _format-char_\n",
        "    for word in corpus:\n",
        "        new_word = re.sub(pattern, ''.join(pair), word) # replace merged_pair trên từng sentence\n",
        "        new_corpus[new_word] = corpus[word]\n",
        "    return new_corpus\n",
        "\n",
        "def get_vocab(corpus):\n",
        "    \"\"\"Lấy ra danh sách tất cả các token từ corpus và tokenize của một word\n",
        "\n",
        "    corpus: {word: freq, word: freq, word: freq,...}\n",
        "    vocab: {token: freq, token: freq, token: freq,...}\n",
        "    known_word_tokenization: {word: word_tokenized, word: word_tokenized,...}\n",
        "\n",
        "    return vocab, known_word_tokenization\n",
        "\n",
        "    \"\"\"\n",
        "    vocab = collections.defaultdict(int)\n",
        "    known_word_tokenization = collections.defaultdict(str)\n",
        "    for word, freq in corpus.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            vocab[token] += freq\n",
        "        known_word_tokenization[word] = word_tokens\n",
        "    return vocab, known_word_tokenization\n",
        "\n",
        "def get_len_token(token):\n",
        "    \"\"\"Lấy ra độ dài của token\n",
        "    \"\"\"\n",
        "    if token[-4:] == '</w>':\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        return len(token)\n",
        "\n",
        "def decode_word(word_tokened):\n",
        "    \"\"\"Decode word đã được token\n",
        "\n",
        "       word_tokened: [token, token, token,...]\n",
        "       return word: string\n",
        "    \"\"\"\n",
        "    word = ''.join(word_tokened)\n",
        "    word_decoded = word.replace('</w>', ' ')\n",
        "    return word_decoded\n",
        "\n",
        "def encode_word(word, sorted_tokens, unk_token='<UNK>'):\n",
        "    \"\"\"Encode word theo vocabulary\n",
        "\n",
        "    word: string chưa được token\n",
        "    sorted_tokens: tập vocabulary các token được sorted DESC\n",
        "    unk_token: unknown token\n",
        "\n",
        "    return word đã được token theo BPE\n",
        "    \"\"\"\n",
        "    if word == '':\n",
        "        return []\n",
        "    if sorted_tokens == []:\n",
        "        return [unk_token]\n",
        "\n",
        "    word_tokened = []\n",
        "    for i in range(len(sorted_tokens)):\n",
        "        token = sorted_tokens[i]\n",
        "        token_reg = re.escape(token)\n",
        "        matched_positions = [(m.start(), m.end()) for m in re.finditer(token_reg, word)]\n",
        "        if len(matched_positions) == 0:\n",
        "            continue\n",
        "        subword_end_positions = [pos[0] for pos in matched_positions]\n",
        "        subword_start_position = 0\n",
        "        for subword_end_position in subword_end_positions:\n",
        "            subword_prev = word[subword_start_position:subword_end_position]\n",
        "            word_tokened += encode_word(subword_prev, sorted_tokens[i+1:])\n",
        "            word_tokened += [token]\n",
        "            subword_start_position = subword_end_position + len(token)\n",
        "        subword_remain = word[subword_start_position:]\n",
        "        word_tokened += encode_word(subword_remain, sorted_tokens[i+1:])\n",
        "        break\n",
        "    return word_tokened"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQGGX1xvMWCV"
      },
      "source": [
        "# đọc file -> lấy corpus\n",
        "corpus = get_corpus('./pg16457.txt')\n",
        "num_merge = 1000\n",
        "\n",
        "# xây dựng tập vocab gồm các token\n",
        "for i in range(num_merge):\n",
        "    pairs = get_stats(corpus)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best_pair = max(pairs, key=pairs.get)\n",
        "    corpus = update_corpus(best_pair, corpus)\n",
        "    vocab, known_word_tokenized = get_vocab(corpus)\n",
        "    print(f'iter: {i}')\n",
        "    print(f'best pair = {best_pair}')\n",
        "    print(f'number of vocab = {len(vocab)}')\n",
        "    print(f'list vocabularies = {vocab}')\n",
        "    print('===================================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-4ev39CNenc"
      },
      "source": [
        "# sort các token theo cả len token và frequency\n",
        "sorted_tokens_tuple = sorted(vocab.items(), \n",
        "                            key=lambda item: (get_len_token(item[0]), item[1]),\n",
        "                            reverse=True)\n",
        "li_tokens_sorted = [token for (token, freq) in sorted_tokens_tuple]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7cRDJBUPLFA"
      },
      "source": [
        "# demo tokenize word theo BPE\n",
        "demo_word = 'Ilikeeatingapples!</w>'\n",
        "if demo_word in known_word_tokenized:\n",
        "    print(f'word tokenized = {demo_word}')\n",
        "else:\n",
        "    word_encoded = encode_word(demo_word, li_tokens_sorted)\n",
        "    print(f'word tokenized = {word_encoded}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}